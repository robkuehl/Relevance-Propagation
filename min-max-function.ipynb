{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597762908960",
   "display_name": "Python 3.7.7 64-bit ('tf_env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.rel_prop.min_max_rel_model import MinMaxModel, Nested_Regressor\n",
    "from src.models.Binary_Mnist_Model import Montavon_Classifier\n",
    "import numpy as np\n",
    "from src.rel_prop.minmax_utils import get_higher_relevances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train images: 21840, train labels: 21840\nLoad model\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten (Flatten)            (None, 784)               0         \n_________________________________________________________________\ndense (Dense)                (None, 400)               313600    \n_________________________________________________________________\ndense_1 (Dense)              (None, 100)               40000     \n_________________________________________________________________\ndense_2 (Dense)              (None, 400)               40000     \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 400       \n=================================================================\nTotal params: 394,000\nTrainable params: 353,600\nNon-trainable params: 40,400\n_________________________________________________________________\nModel has been load, no need to train!\n"
    }
   ],
   "source": [
    "mc = Montavon_Classifier(class_nb=8, load_model=True)\n",
    "mc.set_data(test_size=0.2)\n",
    "mc.set_model()\n",
    "mc.model.summary()\n",
    "mc.fit_model(epochs=300, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = get_higher_relevances(classifier=mc, recalc_rel=True, use_higher_rel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Started to collect relevances to train min-max-model!\nInfo: You decided not to use higher relevances for training.\nLoad relevances to train min-max-model from local directory!\nCreated MinMaxModel\n"
    }
   ],
   "source": [
    "minmax = MinMaxModel(classifier=mc, use_higher_rel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nimage_input (InputLayer)        [(None, 28, 28)]     0                                            \n__________________________________________________________________________________________________\nbias_input (InputLayer)         [(None, 400, 1)]     0                                            \n__________________________________________________________________________________________________\nflat_image (Flatten)            (None, 784)          0           image_input[0][0]                \n__________________________________________________________________________________________________\nflat_bias (Flatten)             (None, 400)          0           bias_input[0][0]                 \n__________________________________________________________________________________________________\ndense_image (Dense)             (None, 100)          78400       flat_image[0][0]                 \n__________________________________________________________________________________________________\ndense_bias (Dense)              (None, 100)          40100       flat_bias[0][0]                  \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 200)          0           dense_image[0][0]                \n                                                                 dense_bias[0][0]                 \n__________________________________________________________________________________________________\nadd_up_layer (Dense)            (None, 100)          20000       concatenate[0][0]                \n__________________________________________________________________________________________________\nsum_pooling (Dense)             (None, 1)            101         add_up_layer[0][0]               \n==================================================================================================\nTotal params: 138,601\nTrainable params: 118,500\nNon-trainable params: 20,101\n__________________________________________________________________________________________________\nNone\nCreated nested regressor for neuron with index 0\n"
    }
   ],
   "source": [
    "from src.rel_prop.min_max_rel_model import Nested_Regressor\n",
    "\n",
    "nr = Nested_Regressor(input_shape=(28,28), use_bias=True, neuron_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = minmax.nr_train_images\n",
    "true_relevances = minmax.true_relevances\n",
    "higher_relevances = np.transpose(minmax.higher_relevances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'numpy.ndarray'>\n(4607, 28, 28)\n(28, 28)\n"
    }
   ],
   "source": [
    "print(type(train_images))\n",
    "print(train_images.shape)\n",
    "print(train_images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'numpy.ndarray'>\n(100, 4607)\n(4607,)\n"
    }
   ],
   "source": [
    "print(type(true_relevances))\n",
    "print(true_relevances.shape)\n",
    "print(true_relevances[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'numpy.ndarray'>\n(4607, 400)\n(400,)\n"
    }
   ],
   "source": [
    "print(type(higher_relevances))\n",
    "print(higher_relevances.shape)\n",
    "print(higher_relevances[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "23 [==============================] - 0s 2ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 6.1990 - val_mse: 6.1990\nEpoch 161/300\n123/123 [==============================] - 0s 2ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 6.1989 - val_mse: 6.1989\nEpoch 162/300\n123/123 [==============================] - 0s 2ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 6.1987 - val_mse: 6.1987\nEpoch 163/300\n123/123 [==============================] - 0s 2ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 6.1985 - val_mse: 6.1985\nEpoch 164/300\n123/123 [==============================] - 0s 2ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 6.1984 - val_mse: 6.1984\nEpoch 165/300\n123/123 [==============================] - 0s 2ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 6.1982 - val_mse: 6.1982\nEpoch 166/300\n123/123 [==============================] - 0s 2ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 6.1981 - val_mse: 6.1981\nEpoch 167/300\n123/123 [==============================] - 0s 2ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 6.1979 - val_mse: 6.1979\nEpoch 168/300\n123/123 [==============================] - 0s 2ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 6.1978 - val_mse: 6.1978\nEpoch 169/300\n123/123 [==============================] - 0s 2ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 6.1977 - val_mse: 6.1977\nEpoch 170/300\n123/123 [==============================] - 0s 2ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 6.1975 - val_mse: 6.1975\nEpoch 171/300\n123/123 [==============================] - 0s 2ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 6.1974 - val_mse: 6.1974\nEpoch 172/300\n123/123 [==============================] - 0s 2ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 6.1973 - val_mse: 6.1973\nEpoch 173/300\n123/123 [==============================] - 0s 2ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 6.1972 - val_mse: 6.1972\nEpoch 174/300\n123/123 [==============================] - 0s 2ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 6.1971 - val_mse: 6.1971\nEpoch 175/300\n123/123 [==============================] - 0s 2ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 6.1970 - val_mse: 6.1970\nEpoch 176/300\n123/123 [==============================] - 0s 2ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 6.1969 - val_mse: 6.1969\nEpoch 177/300\n123/123 [==============================] - 0s 2ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 6.1968 - val_mse: 6.1968\nEpoch 178/300\n123/123 [==============================] - 0s 2ms/step - loss: 9.8726e-04 - mse: 9.8726e-04 - val_loss: 6.1967 - val_mse: 6.1967\nEpoch 179/300\n123/123 [==============================] - 0s 2ms/step - loss: 9.5072e-04 - mse: 9.5072e-04 - val_loss: 6.1966 - val_mse: 6.1966\nEpoch 180/300\n123/123 [==============================] - 0s 2ms/step - loss: 9.1559e-04 - mse: 9.1559e-04 - val_loss: 6.1965 - val_mse: 6.1965\nEpoch 181/300\n123/123 [==============================] - 0s 2ms/step - loss: 8.8180e-04 - mse: 8.8180e-04 - val_loss: 6.1964 - val_mse: 6.1964\nEpoch 182/300\n123/123 [==============================] - 0s 2ms/step - loss: 8.4931e-04 - mse: 8.4931e-04 - val_loss: 6.1964 - val_mse: 6.1964\nEpoch 183/300\n123/123 [==============================] - 0s 2ms/step - loss: 8.1805e-04 - mse: 8.1805e-04 - val_loss: 6.1963 - val_mse: 6.1963\nEpoch 184/300\n123/123 [==============================] - 0s 2ms/step - loss: 7.8800e-04 - mse: 7.8800e-04 - val_loss: 6.1962 - val_mse: 6.1962\nEpoch 185/300\n123/123 [==============================] - 0s 2ms/step - loss: 7.5909e-04 - mse: 7.5909e-04 - val_loss: 6.1961 - val_mse: 6.1961\nEpoch 186/300\n123/123 [==============================] - 0s 2ms/step - loss: 7.3128e-04 - mse: 7.3128e-04 - val_loss: 6.1961 - val_mse: 6.1961\nEpoch 187/300\n123/123 [==============================] - 0s 2ms/step - loss: 7.0452e-04 - mse: 7.0452e-04 - val_loss: 6.1960 - val_mse: 6.1960\nEpoch 188/300\n123/123 [==============================] - 0s 2ms/step - loss: 6.7878e-04 - mse: 6.7878e-04 - val_loss: 6.1959 - val_mse: 6.1959\nEpoch 189/300\n123/123 [==============================] - 0s 2ms/step - loss: 6.5401e-04 - mse: 6.5401e-04 - val_loss: 6.1959 - val_mse: 6.1959\nEpoch 190/300\n123/123 [==============================] - 0s 2ms/step - loss: 6.3018e-04 - mse: 6.3018e-04 - val_loss: 6.1958 - val_mse: 6.1958\nEpoch 191/300\n123/123 [==============================] - 0s 2ms/step - loss: 6.0725e-04 - mse: 6.0725e-04 - val_loss: 6.1958 - val_mse: 6.1958\nEpoch 192/300\n123/123 [==============================] - 0s 2ms/step - loss: 5.8519e-04 - mse: 5.8519e-04 - val_loss: 6.1957 - val_mse: 6.1957\nEpoch 193/300\n123/123 [==============================] - 0s 2ms/step - loss: 5.6395e-04 - mse: 5.6395e-04 - val_loss: 6.1957 - val_mse: 6.1957\nEpoch 194/300\n123/123 [==============================] - 0s 2ms/step - loss: 5.4350e-04 - mse: 5.4350e-04 - val_loss: 6.1956 - val_mse: 6.1956\nEpoch 195/300\n123/123 [==============================] - 0s 2ms/step - loss: 5.2382e-04 - mse: 5.2382e-04 - val_loss: 6.1956 - val_mse: 6.1956\nEpoch 196/300\n123/123 [==============================] - 0s 2ms/step - loss: 5.0489e-04 - mse: 5.0489e-04 - val_loss: 6.1955 - val_mse: 6.1955\nEpoch 197/300\n123/123 [==============================] - 0s 2ms/step - loss: 4.8666e-04 - mse: 4.8666e-04 - val_loss: 6.1955 - val_mse: 6.1955\nEpoch 198/300\n123/123 [==============================] - 0s 2ms/step - loss: 4.6911e-04 - mse: 4.6911e-04 - val_loss: 6.1954 - val_mse: 6.1954\nEpoch 199/300\n123/123 [==============================] - 0s 2ms/step - loss: 4.5221e-04 - mse: 4.5221e-04 - val_loss: 6.1954 - val_mse: 6.1954\nEpoch 200/300\n123/123 [==============================] - 0s 2ms/step - loss: 4.3594e-04 - mse: 4.3594e-04 - val_loss: 6.1954 - val_mse: 6.1954\nEpoch 201/300\n123/123 [==============================] - 0s 2ms/step - loss: 3.9997e-04 - mse: 3.9997e-04 - val_loss: 6.1953 - val_mse: 6.1953\nEpoch 202/300\n123/123 [==============================] - 0s 2ms/step - loss: 3.8556e-04 - mse: 3.8556e-04 - val_loss: 6.1953 - val_mse: 6.1953\nEpoch 203/300\n123/123 [==============================] - 0s 2ms/step - loss: 3.7168e-04 - mse: 3.7168e-04 - val_loss: 6.1953 - val_mse: 6.1953\nEpoch 204/300\n123/123 [==============================] - 0s 2ms/step - loss: 3.5831e-04 - mse: 3.5831e-04 - val_loss: 6.1952 - val_mse: 6.1952\nEpoch 205/300\n123/123 [==============================] - 0s 2ms/step - loss: 3.4545e-04 - mse: 3.4545e-04 - val_loss: 6.1952 - val_mse: 6.1952\nEpoch 206/300\n123/123 [==============================] - 0s 2ms/step - loss: 3.3306e-04 - mse: 3.3306e-04 - val_loss: 6.1952 - val_mse: 6.1952\nEpoch 207/300\n123/123 [==============================] - 0s 2ms/step - loss: 3.2113e-04 - mse: 3.2113e-04 - val_loss: 6.1951 - val_mse: 6.1951\nEpoch 208/300\n123/123 [==============================] - 0s 2ms/step - loss: 3.0964e-04 - mse: 3.0964e-04 - val_loss: 6.1951 - val_mse: 6.1951\nEpoch 209/300\n123/123 [==============================] - 0s 2ms/step - loss: 2.9858e-04 - mse: 2.9858e-04 - val_loss: 6.1951 - val_mse: 6.1951\nEpoch 210/300\n123/123 [==============================] - 0s 2ms/step - loss: 2.8792e-04 - mse: 2.8792e-04 - val_loss: 6.1950 - val_mse: 6.1950\nEpoch 211/300\n123/123 [==============================] - 0s 2ms/step - loss: 2.7766e-04 - mse: 2.7766e-04 - val_loss: 6.1950 - val_mse: 6.1950\nEpoch 212/300\n123/123 [==============================] - 0s 2ms/step - loss: 2.6776e-04 - mse: 2.6776e-04 - val_loss: 6.1950 - val_mse: 6.1950\nEpoch 213/300\n123/123 [==============================] - 0s 2ms/step - loss: 2.5824e-04 - mse: 2.5824e-04 - val_loss: 6.1950 - val_mse: 6.1950\nEpoch 214/300\n123/123 [==============================] - 0s 2ms/step - loss: 2.4906e-04 - mse: 2.4906e-04 - val_loss: 6.1950 - val_mse: 6.1950\nEpoch 215/300\n123/123 [==============================] - 0s 2ms/step - loss: 2.4021e-04 - mse: 2.4021e-04 - val_loss: 6.1949 - val_mse: 6.1949\nEpoch 216/300\n123/123 [==============================] - 0s 2ms/step - loss: 2.3170e-04 - mse: 2.3170e-04 - val_loss: 6.1949 - val_mse: 6.1949\nEpoch 217/300\n123/123 [==============================] - 0s 2ms/step - loss: 2.2349e-04 - mse: 2.2349e-04 - val_loss: 6.1949 - val_mse: 6.1949\nEpoch 218/300\n123/123 [==============================] - 0s 2ms/step - loss: 2.1558e-04 - mse: 2.1558e-04 - val_loss: 6.1949 - val_mse: 6.1949\nEpoch 219/300\n123/123 [==============================] - 0s 2ms/step - loss: 2.0796e-04 - mse: 2.0796e-04 - val_loss: 6.1948 - val_mse: 6.1948\nEpoch 220/300\n123/123 [==============================] - 0s 2ms/step - loss: 2.0062e-04 - mse: 2.0062e-04 - val_loss: 6.1948 - val_mse: 6.1948\nEpoch 221/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.9354e-04 - mse: 1.9354e-04 - val_loss: 6.1948 - val_mse: 6.1948\nEpoch 222/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.8672e-04 - mse: 1.8672e-04 - val_loss: 6.1948 - val_mse: 6.1948\nEpoch 223/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.8015e-04 - mse: 1.8015e-04 - val_loss: 6.1948 - val_mse: 6.1948\nEpoch 224/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.7381e-04 - mse: 1.7381e-04 - val_loss: 6.1948 - val_mse: 6.1948\nEpoch 225/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.6770e-04 - mse: 1.6770e-04 - val_loss: 6.1947 - val_mse: 6.1947\nEpoch 226/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.6181e-04 - mse: 1.6181e-04 - val_loss: 6.1947 - val_mse: 6.1947\nEpoch 227/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.5614e-04 - mse: 1.5614e-04 - val_loss: 6.1947 - val_mse: 6.1947\nEpoch 228/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.5067e-04 - mse: 1.5067e-04 - val_loss: 6.1947 - val_mse: 6.1947\nEpoch 229/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.4540e-04 - mse: 1.4540e-04 - val_loss: 6.1947 - val_mse: 6.1947\nEpoch 230/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.4031e-04 - mse: 1.4031e-04 - val_loss: 6.1947 - val_mse: 6.1947\nEpoch 231/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.3542e-04 - mse: 1.3542e-04 - val_loss: 6.1947 - val_mse: 6.1947\nEpoch 232/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.3069e-04 - mse: 1.3069e-04 - val_loss: 6.1947 - val_mse: 6.1947\nEpoch 233/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.2613e-04 - mse: 1.2613e-04 - val_loss: 6.1947 - val_mse: 6.1947\nEpoch 234/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.2174e-04 - mse: 1.2174e-04 - val_loss: 6.1946 - val_mse: 6.1946\nEpoch 235/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.1750e-04 - mse: 1.1750e-04 - val_loss: 6.1946 - val_mse: 6.1946\nEpoch 236/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.1342e-04 - mse: 1.1342e-04 - val_loss: 6.1946 - val_mse: 6.1946\nEpoch 237/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.0948e-04 - mse: 1.0948e-04 - val_loss: 6.1946 - val_mse: 6.1946\nEpoch 238/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.0569e-04 - mse: 1.0569e-04 - val_loss: 6.1946 - val_mse: 6.1946\nEpoch 239/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.0199e-04 - mse: 1.0199e-04 - val_loss: 6.1946 - val_mse: 6.1946\nEpoch 240/300\n123/123 [==============================] - 0s 2ms/step - loss: 9.8461e-05 - mse: 9.8461e-05 - val_loss: 6.1946 - val_mse: 6.1946\nEpoch 241/300\n123/123 [==============================] - 0s 2ms/step - loss: 9.5054e-05 - mse: 9.5054e-05 - val_loss: 6.1946 - val_mse: 6.1946\nEpoch 242/300\n123/123 [==============================] - 0s 2ms/step - loss: 9.1773e-05 - mse: 9.1773e-05 - val_loss: 6.1946 - val_mse: 6.1946\nEpoch 243/300\n123/123 [==============================] - 0s 2ms/step - loss: 8.8605e-05 - mse: 8.8605e-05 - val_loss: 6.1946 - val_mse: 6.1946\nEpoch 244/300\n123/123 [==============================] - 0s 2ms/step - loss: 8.5549e-05 - mse: 8.5549e-05 - val_loss: 6.1946 - val_mse: 6.1946\nEpoch 245/300\n123/123 [==============================] - 0s 2ms/step - loss: 8.2602e-05 - mse: 8.2602e-05 - val_loss: 6.1946 - val_mse: 6.1946\nEpoch 246/300\n123/123 [==============================] - 0s 2ms/step - loss: 7.9763e-05 - mse: 7.9763e-05 - val_loss: 6.1946 - val_mse: 6.1946\nEpoch 247/300\n123/123 [==============================] - 0s 2ms/step - loss: 7.7019e-05 - mse: 7.7019e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 248/300\n123/123 [==============================] - 0s 2ms/step - loss: 7.4376e-05 - mse: 7.4376e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 249/300\n123/123 [==============================] - 0s 2ms/step - loss: 7.1823e-05 - mse: 7.1823e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 250/300\n123/123 [==============================] - 0s 2ms/step - loss: 6.9362e-05 - mse: 6.9362e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 251/300\n123/123 [==============================] - 0s 2ms/step - loss: 6.6988e-05 - mse: 6.6988e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 252/300\n123/123 [==============================] - 0s 2ms/step - loss: 6.4700e-05 - mse: 6.4700e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 253/300\n123/123 [==============================] - 0s 2ms/step - loss: 6.2488e-05 - mse: 6.2488e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 254/300\n123/123 [==============================] - 0s 2ms/step - loss: 6.0357e-05 - mse: 6.0357e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 255/300\n123/123 [==============================] - 0s 2ms/step - loss: 5.8301e-05 - mse: 5.8301e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 256/300\n123/123 [==============================] - 0s 2ms/step - loss: 5.6316e-05 - mse: 5.6316e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 257/300\n123/123 [==============================] - 0s 2ms/step - loss: 5.4401e-05 - mse: 5.4401e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 258/300\n123/123 [==============================] - 0s 2ms/step - loss: 5.2555e-05 - mse: 5.2555e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 259/300\n123/123 [==============================] - 0s 2ms/step - loss: 5.0770e-05 - mse: 5.0770e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 260/300\n123/123 [==============================] - 0s 2ms/step - loss: 4.9049e-05 - mse: 4.9049e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 261/300\n123/123 [==============================] - 0s 2ms/step - loss: 4.7391e-05 - mse: 4.7391e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 262/300\n123/123 [==============================] - 0s 2ms/step - loss: 4.5787e-05 - mse: 4.5787e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 263/300\n123/123 [==============================] - 0s 2ms/step - loss: 4.4242e-05 - mse: 4.4242e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 264/300\n123/123 [==============================] - 0s 2ms/step - loss: 4.2749e-05 - mse: 4.2749e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 265/300\n123/123 [==============================] - 0s 2ms/step - loss: 4.1309e-05 - mse: 4.1309e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 266/300\n123/123 [==============================] - 0s 2ms/step - loss: 3.9920e-05 - mse: 3.9920e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 267/300\n123/123 [==============================] - 0s 2ms/step - loss: 3.8578e-05 - mse: 3.8578e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 268/300\n123/123 [==============================] - 0s 2ms/step - loss: 3.7285e-05 - mse: 3.7285e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 269/300\n123/123 [==============================] - 0s 2ms/step - loss: 3.6034e-05 - mse: 3.6034e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 270/300\n123/123 [==============================] - 0s 2ms/step - loss: 3.4829e-05 - mse: 3.4829e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 271/300\n123/123 [==============================] - 0s 2ms/step - loss: 3.3663e-05 - mse: 3.3663e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 272/300\n123/123 [==============================] - 0s 2ms/step - loss: 3.2540e-05 - mse: 3.2540e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 273/300\n123/123 [==============================] - 0s 2ms/step - loss: 3.1456e-05 - mse: 3.1456e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 274/300\n123/123 [==============================] - 0s 2ms/step - loss: 3.0408e-05 - mse: 3.0408e-05 - val_loss: 6.1945 - val_mse: 6.1945\nEpoch 275/300\n123/123 [==============================] - 0s 2ms/step - loss: 2.9398e-05 - mse: 2.9398e-05 - val_loss: 6.1944 - val_mse: 6.1944\nEpoch 276/300\n123/123 [==============================] - 0s 2ms/step - loss: 2.8422e-05 - mse: 2.8422e-05 - val_loss: 6.1944 - val_mse: 6.1944\nEpoch 277/300\n123/123 [==============================] - 0s 2ms/step - loss: 2.7479e-05 - mse: 2.7479e-05 - val_loss: 6.1944 - val_mse: 6.1944\nEpoch 278/300\n123/123 [==============================] - 0s 2ms/step - loss: 2.6569e-05 - mse: 2.6569e-05 - val_loss: 6.1944 - val_mse: 6.1944\nEpoch 279/300\n123/123 [==============================] - 0s 2ms/step - loss: 2.5693e-05 - mse: 2.5693e-05 - val_loss: 6.1944 - val_mse: 6.1944\nEpoch 280/300\n123/123 [==============================] - 0s 2ms/step - loss: 2.4845e-05 - mse: 2.4845e-05 - val_loss: 6.1944 - val_mse: 6.1944\nEpoch 281/300\n123/123 [==============================] - 0s 2ms/step - loss: 2.4024e-05 - mse: 2.4024e-05 - val_loss: 6.1944 - val_mse: 6.1944\nEpoch 282/300\n123/123 [==============================] - 0s 2ms/step - loss: 2.3234e-05 - mse: 2.3234e-05 - val_loss: 6.1944 - val_mse: 6.1944\nEpoch 283/300\n123/123 [==============================] - 0s 2ms/step - loss: 2.2473e-05 - mse: 2.2473e-05 - val_loss: 6.1944 - val_mse: 6.1944\nEpoch 284/300\n123/123 [==============================] - 0s 2ms/step - loss: 2.1735e-05 - mse: 2.1735e-05 - val_loss: 6.1944 - val_mse: 6.1944\nEpoch 285/300\n123/123 [==============================] - 0s 2ms/step - loss: 2.1027e-05 - mse: 2.1027e-05 - val_loss: 6.1944 - val_mse: 6.1944\nEpoch 286/300\n123/123 [==============================] - 0s 2ms/step - loss: 2.0340e-05 - mse: 2.0340e-05 - val_loss: 6.1944 - val_mse: 6.1944\nEpoch 287/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.9678e-05 - mse: 1.9678e-05 - val_loss: 6.1944 - val_mse: 6.1944\nEpoch 288/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.9039e-05 - mse: 1.9039e-05 - val_loss: 6.1944 - val_mse: 6.1944\nEpoch 289/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.8421e-05 - mse: 1.8421e-05 - val_loss: 6.1944 - val_mse: 6.1944\nEpoch 290/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.7825e-05 - mse: 1.7825e-05 - val_loss: 6.1944 - val_mse: 6.1944\nEpoch 291/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.7249e-05 - mse: 1.7249e-05 - val_loss: 6.1944 - val_mse: 6.1944\nEpoch 292/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.6695e-05 - mse: 1.6695e-05 - val_loss: 6.1944 - val_mse: 6.1944\nEpoch 293/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.6158e-05 - mse: 1.6158e-05 - val_loss: 6.1944 - val_mse: 6.1944\nEpoch 294/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.5640e-05 - mse: 1.5640e-05 - val_loss: 6.1944 - val_mse: 6.1944\nEpoch 295/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.5139e-05 - mse: 1.5139e-05 - val_loss: 6.1944 - val_mse: 6.1944\nEpoch 296/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.4656e-05 - mse: 1.4656e-05 - val_loss: 6.1944 - val_mse: 6.1944\nEpoch 297/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.4190e-05 - mse: 1.4190e-05 - val_loss: 6.1944 - val_mse: 6.1944\nEpoch 298/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.3739e-05 - mse: 1.3739e-05 - val_loss: 6.1944 - val_mse: 6.1944\nEpoch 299/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.3304e-05 - mse: 1.3304e-05 - val_loss: 6.1944 - val_mse: 6.1944\nEpoch 300/300\n123/123 [==============================] - 0s 2ms/step - loss: 1.2884e-05 - mse: 1.2884e-05 - val_loss: 6.1944 - val_mse: 6.1944\n"
    }
   ],
   "source": [
    "nr.fit_approx_model(train_images=train_images, true_relevances=true_relevances[0], higher_relevances=higher_relevances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}